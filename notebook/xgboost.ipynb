{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, roc_curve\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Score</th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col3</th>\n",
       "      <th>col4</th>\n",
       "      <th>col5</th>\n",
       "      <th>col6</th>\n",
       "      <th>col7</th>\n",
       "      <th>col8</th>\n",
       "      <th>...</th>\n",
       "      <th>col3796</th>\n",
       "      <th>col3797</th>\n",
       "      <th>col3798</th>\n",
       "      <th>col3799</th>\n",
       "      <th>col3800</th>\n",
       "      <th>col3801</th>\n",
       "      <th>col3802</th>\n",
       "      <th>col3803</th>\n",
       "      <th>col3804</th>\n",
       "      <th>col3805</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.475628</td>\n",
       "      <td>0</td>\n",
       "      <td>4.058</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0</td>\n",
       "      <td>10.267</td>\n",
       "      <td>0.728</td>\n",
       "      <td>4.403</td>\n",
       "      <td>0.050</td>\n",
       "      <td>...</td>\n",
       "      <td>1.067</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.115</td>\n",
       "      <td>30.395</td>\n",
       "      <td>24.541</td>\n",
       "      <td>0</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>3.601332</td>\n",
       "      <td>0</td>\n",
       "      <td>4.111</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0</td>\n",
       "      <td>8.352</td>\n",
       "      <td>0.907</td>\n",
       "      <td>4.216</td>\n",
       "      <td>0.034</td>\n",
       "      <td>...</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.227</td>\n",
       "      <td>38.508</td>\n",
       "      <td>35.038</td>\n",
       "      <td>0</td>\n",
       "      <td>3.979</td>\n",
       "      <td>0.997</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1.935003</td>\n",
       "      <td>0</td>\n",
       "      <td>4.139</td>\n",
       "      <td>0.833</td>\n",
       "      <td>66</td>\n",
       "      <td>9.494</td>\n",
       "      <td>0.733</td>\n",
       "      <td>4.069</td>\n",
       "      <td>0.267</td>\n",
       "      <td>...</td>\n",
       "      <td>1.722</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.148</td>\n",
       "      <td>27.932</td>\n",
       "      <td>19.518</td>\n",
       "      <td>0</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>3.283663</td>\n",
       "      <td>0</td>\n",
       "      <td>4.016</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0</td>\n",
       "      <td>8.237</td>\n",
       "      <td>0.836</td>\n",
       "      <td>3.956</td>\n",
       "      <td>0.129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.124</td>\n",
       "      <td>18.993</td>\n",
       "      <td>25.403</td>\n",
       "      <td>0</td>\n",
       "      <td>0.988</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>3.409121</td>\n",
       "      <td>0</td>\n",
       "      <td>4.657</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0</td>\n",
       "      <td>35.882</td>\n",
       "      <td>0.383</td>\n",
       "      <td>4.234</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>...</td>\n",
       "      <td>2.095</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.088</td>\n",
       "      <td>44.225</td>\n",
       "      <td>15.741</td>\n",
       "      <td>0</td>\n",
       "      <td>1.595</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13726</td>\n",
       "      <td>27450</td>\n",
       "      <td>1.897627</td>\n",
       "      <td>0</td>\n",
       "      <td>4.232</td>\n",
       "      <td>0.720</td>\n",
       "      <td>6</td>\n",
       "      <td>14.717</td>\n",
       "      <td>0.767</td>\n",
       "      <td>4.166</td>\n",
       "      <td>0.183</td>\n",
       "      <td>...</td>\n",
       "      <td>1.556</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111</td>\n",
       "      <td>12.433</td>\n",
       "      <td>17.890</td>\n",
       "      <td>0</td>\n",
       "      <td>1.061</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13727</td>\n",
       "      <td>27453</td>\n",
       "      <td>1.567026</td>\n",
       "      <td>0</td>\n",
       "      <td>4.389</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0</td>\n",
       "      <td>21.853</td>\n",
       "      <td>0.667</td>\n",
       "      <td>4.193</td>\n",
       "      <td>0.017</td>\n",
       "      <td>...</td>\n",
       "      <td>1.556</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>39.003</td>\n",
       "      <td>21.724</td>\n",
       "      <td>0</td>\n",
       "      <td>1.929</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13728</td>\n",
       "      <td>27459</td>\n",
       "      <td>4.008115</td>\n",
       "      <td>0</td>\n",
       "      <td>4.510</td>\n",
       "      <td>0.817</td>\n",
       "      <td>6</td>\n",
       "      <td>25.947</td>\n",
       "      <td>0.688</td>\n",
       "      <td>4.326</td>\n",
       "      <td>0.232</td>\n",
       "      <td>...</td>\n",
       "      <td>2.292</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.081</td>\n",
       "      <td>101.934</td>\n",
       "      <td>31.747</td>\n",
       "      <td>0</td>\n",
       "      <td>2.716</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13729</td>\n",
       "      <td>27460</td>\n",
       "      <td>0.255273</td>\n",
       "      <td>0</td>\n",
       "      <td>4.490</td>\n",
       "      <td>0.689</td>\n",
       "      <td>0</td>\n",
       "      <td>25.385</td>\n",
       "      <td>0.497</td>\n",
       "      <td>4.713</td>\n",
       "      <td>0.059</td>\n",
       "      <td>...</td>\n",
       "      <td>2.227</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.079</td>\n",
       "      <td>37.596</td>\n",
       "      <td>19.484</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13730</td>\n",
       "      <td>27463</td>\n",
       "      <td>2.321184</td>\n",
       "      <td>0</td>\n",
       "      <td>4.044</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0</td>\n",
       "      <td>5.564</td>\n",
       "      <td>0.563</td>\n",
       "      <td>4.548</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.127</td>\n",
       "      <td>8.514</td>\n",
       "      <td>13.845</td>\n",
       "      <td>0</td>\n",
       "      <td>2.154</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13731 rows × 3807 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID     Score  col1   col2   col3  col4    col5   col6   col7   col8  \\\n",
       "0          5  3.475628     0  4.058  0.824     0  10.267  0.728  4.403  0.050   \n",
       "1          8  3.601332     0  4.111  0.929     0   8.352  0.907  4.216  0.034   \n",
       "2          9  1.935003     0  4.139  0.833    66   9.494  0.733  4.069  0.267   \n",
       "3         12  3.283663     0  4.016  0.880     0   8.237  0.836  3.956  0.129   \n",
       "4         14  3.409121     0  4.657  0.522     0  35.882  0.383  4.234 -0.089   \n",
       "...      ...       ...   ...    ...    ...   ...     ...    ...    ...    ...   \n",
       "13726  27450  1.897627     0  4.232  0.720     6  14.717  0.767  4.166  0.183   \n",
       "13727  27453  1.567026     0  4.389  0.785     0  21.853  0.667  4.193  0.017   \n",
       "13728  27459  4.008115     0  4.510  0.817     6  25.947  0.688  4.326  0.232   \n",
       "13729  27460  0.255273     0  4.490  0.689     0  25.385  0.497  4.713  0.059   \n",
       "13730  27463  2.321184     0  4.044  0.665     0   5.564  0.563  4.548 -0.015   \n",
       "\n",
       "       ...  col3796  col3797  col3798  col3799  col3800  col3801  col3802  \\\n",
       "0      ...    1.067        0      0.0    0.115   30.395   24.541        0   \n",
       "1      ...    0.934        0      0.0    0.227   38.508   35.038        0   \n",
       "2      ...    1.722        0      0.0    0.148   27.932   19.518        0   \n",
       "3      ...    0.993        0      0.0    0.124   18.993   25.403        0   \n",
       "4      ...    2.095        0      0.0    0.088   44.225   15.741        0   \n",
       "...    ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "13726  ...    1.556        0      0.0    0.111   12.433   17.890        0   \n",
       "13727  ...    1.556        0      0.0    0.125   39.003   21.724        0   \n",
       "13728  ...    2.292        1      0.0    0.081  101.934   31.747        0   \n",
       "13729  ...    2.227        0      0.0    0.079   37.596   19.484        0   \n",
       "13730  ...    0.868        0      0.0    0.127    8.514   13.845        0   \n",
       "\n",
       "       col3803  col3804  col3805  \n",
       "0        0.415    0.997        0  \n",
       "1        3.979    0.997        3  \n",
       "2        0.849    0.999        0  \n",
       "3        0.988    0.998        0  \n",
       "4        1.595    0.997        0  \n",
       "...        ...      ...      ...  \n",
       "13726    1.061    0.997        0  \n",
       "13727    1.929    0.998        0  \n",
       "13728    2.716    0.998        0  \n",
       "13729   -0.248    0.998        0  \n",
       "13730    2.154    0.997        0  \n",
       "\n",
       "[13731 rows x 3807 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_data_path =\"./../Data/train/train.csv\"\n",
    "test_data_path = \"./../Data/test/test.csv\"\n",
    "\n",
    "dataset = pd.read_csv(train_data_path,skipinitialspace=True)\n",
    "test_data = pd.read_csv(test_data_path,skipinitialspace=True)\n",
    "\n",
    "\n",
    "train_dataset = dataset.sample(frac=0.7, random_state=0)\n",
    "test_dataset = dataset.drop(train_dataset.index)\n",
    "        \n",
    "dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9612,)\n"
     ]
    }
   ],
   "source": [
    "# describe for Normalization\n",
    "label = \"Score\"\n",
    "train_stats = train_dataset.describe()\n",
    "train_stats.pop(label)\n",
    "train_stats = train_stats.transpose()\n",
    "\n",
    "test_stats = test_dataset.describe()\n",
    "test_stats.pop(label)\n",
    "test_stats = test_stats.transpose()\n",
    "\n",
    "test_data_stats = test_data.describe()\n",
    "test_data_stats = test_data_stats.transpose()\n",
    "\n",
    "# pop label\n",
    "train_labels = train_dataset.pop(label)\n",
    "test_labels = test_dataset.pop(label)\n",
    "\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9612, 3806)\n"
     ]
    }
   ],
   "source": [
    "# Normalization\n",
    "def norm(train_dataset,train_stats):\n",
    "    return (train_dataset - train_stats[\"mean\"]) / train_stats[\"std\"]\n",
    "\n",
    "normed_train_data = norm(train_dataset,train_stats)\n",
    "normed_test_data = norm(test_dataset,test_stats)\n",
    "# normed_train_data = normed_train_data.dropna(axis=1)\n",
    "# normed_test_data = normed_test_data.dropna(axis=1)\n",
    "\n",
    "normed_train_data.fillna(0, inplace=True)\n",
    "normed_test_data.fillna(0, inplace=True)\n",
    "normed_test_data = norm(test_data,test_data_stats)\n",
    "normed_test_data.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "print(normed_train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 96 candidates, totalling 288 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  9.7min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 54.4min\n",
      "[Parallel(n_jobs=-1)]: Done 288 out of 288 | elapsed: 275.5min finished\n",
      "C:\\Users\\phamduy\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:29:03] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "# 必要なライブラリのインポート\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# 動かすパラメータを明示的に表示\n",
    "# params = {\"learning_rate\":[0.1,0.3,0.5],\n",
    "#         \"max_depth\": [2,3,5,10],\n",
    "#          \"subsample\":[0.5,0.8,0.9,1],\n",
    "#          \"colsample_bytree\": [0.5,1.0],\n",
    "#          }\n",
    "params = {\"learning_rate\":[0.1,0.3],\n",
    "        \"max_depth\": [2,5,10],\n",
    "         \"subsample\":[0.5,0.8,1],\n",
    "         \"colsample_bytree\": [0.5,1.0],\n",
    "         }\n",
    "# モデルにインスタンス生成\n",
    "mod = xgb.XGBRegressor()\n",
    "# ハイパーパラメータ探索\n",
    "cv = GridSearchCV(mod, params, cv = 3, scoring= 'r2', n_jobs =-1,verbose=2)\n",
    "\n",
    "#　trainデータとtestデータに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(normed_train_data, train_labels, test_size = 0.3, random_state = 0)\n",
    "\n",
    "\n",
    "# 予測モデルを作成\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "#予測    \n",
    "y_train_pred = cv.predict(X_train)\n",
    "y_test_pred = cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score 0.7967299691501538\n",
      "test score 0.5311516946305856\n",
      "0.49862647765733237\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "print(\"train score {}\".format(r2_score(np.array(y_train), y_train_pred)))\n",
    "print(\"test score {}\".format(r2_score(np.array(y_test), y_test_pred)))\n",
    "print(cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearch_table_plot(grid_clf, param_name,\n",
    "                          num_results=15,\n",
    "                          negative=True,\n",
    "                          graph=True,\n",
    "                          display_all_params=True):\n",
    "\n",
    "    '''Display grid search results\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "\n",
    "    grid_clf           the estimator resulting from a grid search\n",
    "                       for example: grid_clf = GridSearchCV( ...\n",
    "\n",
    "    param_name         a string with the name of the parameter being tested\n",
    "\n",
    "    num_results        an integer indicating the number of results to display\n",
    "                       Default: 15\n",
    "\n",
    "    negative           boolean: should the sign of the score be reversed?\n",
    "                       scoring = 'neg_log_loss', for instance\n",
    "                       Default: True\n",
    "\n",
    "    graph              boolean: should a graph be produced?\n",
    "                       non-numeric parameters (True/False, None) don't graph well\n",
    "                       Default: True\n",
    "\n",
    "    display_all_params boolean: should we print out all of the parameters, not just the ones searched for?\n",
    "                       Default: True\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "\n",
    "    GridSearch_table_plot(grid_clf, \"min_samples_leaf\")\n",
    "\n",
    "                          '''\n",
    "    from matplotlib      import pyplot as plt\n",
    "    from IPython.display import display\n",
    "    import pandas as pd\n",
    "\n",
    "    clf = grid_clf.best_estimator_\n",
    "    clf_params = grid_clf.best_params_\n",
    "    if negative:\n",
    "        clf_score = -grid_clf.best_score_\n",
    "    else:\n",
    "        clf_score = grid_clf.best_score_\n",
    "    clf_stdev = grid_clf.cv_results_['std_test_score'][grid_clf.best_index_]\n",
    "    cv_results = grid_clf.cv_results_\n",
    "\n",
    "    print(\"best parameters: {}\".format(clf_params))\n",
    "    print(\"best score:      {:0.5f} (+/-{:0.5f})\".format(clf_score, clf_stdev))\n",
    "    if display_all_params:\n",
    "        import pprint\n",
    "        pprint.pprint(clf.get_params())\n",
    "\n",
    "    # pick out the best results\n",
    "    # =========================\n",
    "    scores_df = pd.DataFrame(cv_results).sort_values(by='rank_test_score')\n",
    "\n",
    "    best_row = scores_df.iloc[0, :]\n",
    "    if negative:\n",
    "        best_mean = -best_row['mean_test_score']\n",
    "    else:\n",
    "        best_mean = best_row['mean_test_score']\n",
    "    best_stdev = best_row['std_test_score']\n",
    "    best_param = best_row['param_' + param_name]\n",
    "\n",
    "    # display the top 'num_results' results\n",
    "    # =====================================\n",
    "    display(pd.DataFrame(cv_results) \\\n",
    "            .sort_values(by='rank_test_score').head(num_results))\n",
    "\n",
    "    # plot the results\n",
    "    # ================\n",
    "    scores_df = scores_df.sort_values(by='param_' + param_name)\n",
    "\n",
    "    if negative:\n",
    "        means = -scores_df['mean_test_score']\n",
    "    else:\n",
    "        means = scores_df['mean_test_score']\n",
    "    stds = scores_df['std_test_score']\n",
    "    params = scores_df['param_' + param_name]\n",
    "\n",
    "    # plot\n",
    "    if graph:\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.errorbar(params, means, yerr=stds)\n",
    "\n",
    "        plt.axhline(y=best_mean + best_stdev, color='red')\n",
    "        plt.axhline(y=best_mean - best_stdev, color='red')\n",
    "        plt.plot(best_param, best_mean, 'or')\n",
    "\n",
    "        plt.title(param_name + \" vs Score\\nBest Score {:0.5f}\".format(clf_score))\n",
    "        plt.xlabel(param_name)\n",
    "        plt.ylabel('Score')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'max_depth': 5, 'subsample': 0.9}\n",
      "best score:      0.49863 (+/-0.00408)\n",
      "{'base_score': 0.5,\n",
      " 'booster': 'gbtree',\n",
      " 'colsample_bylevel': 1,\n",
      " 'colsample_bynode': 1,\n",
      " 'colsample_bytree': 0.5,\n",
      " 'gamma': 0,\n",
      " 'importance_type': 'gain',\n",
      " 'learning_rate': 0.1,\n",
      " 'max_delta_step': 0,\n",
      " 'max_depth': 5,\n",
      " 'min_child_weight': 1,\n",
      " 'missing': None,\n",
      " 'n_estimators': 100,\n",
      " 'n_jobs': 1,\n",
      " 'nthread': None,\n",
      " 'objective': 'reg:linear',\n",
      " 'random_state': 0,\n",
      " 'reg_alpha': 0,\n",
      " 'reg_lambda': 1,\n",
      " 'scale_pos_weight': 1,\n",
      " 'seed': None,\n",
      " 'silent': None,\n",
      " 'subsample': 0.9,\n",
      " 'verbosity': 1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_colsample_bytree</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_subsample</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>85.693844</td>\n",
       "      <td>2.213753</td>\n",
       "      <td>0.544690</td>\n",
       "      <td>0.020821</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>{'colsample_bytree': 0.5, 'learning_rate': 0.1...</td>\n",
       "      <td>0.499862</td>\n",
       "      <td>0.493125</td>\n",
       "      <td>0.502894</td>\n",
       "      <td>0.498626</td>\n",
       "      <td>0.004083</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>89.449371</td>\n",
       "      <td>0.336864</td>\n",
       "      <td>0.525035</td>\n",
       "      <td>0.016968</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>{'colsample_bytree': 0.5, 'learning_rate': 0.1...</td>\n",
       "      <td>0.504106</td>\n",
       "      <td>0.496403</td>\n",
       "      <td>0.492636</td>\n",
       "      <td>0.497716</td>\n",
       "      <td>0.004773</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>145.543646</td>\n",
       "      <td>2.261844</td>\n",
       "      <td>0.500049</td>\n",
       "      <td>0.004782</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>{'colsample_bytree': 1.0, 'learning_rate': 0.1...</td>\n",
       "      <td>0.513539</td>\n",
       "      <td>0.487369</td>\n",
       "      <td>0.491939</td>\n",
       "      <td>0.497617</td>\n",
       "      <td>0.011414</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>145.769618</td>\n",
       "      <td>0.791158</td>\n",
       "      <td>0.524368</td>\n",
       "      <td>0.032047</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>{'colsample_bytree': 1.0, 'learning_rate': 0.1...</td>\n",
       "      <td>0.500517</td>\n",
       "      <td>0.488827</td>\n",
       "      <td>0.501516</td>\n",
       "      <td>0.496953</td>\n",
       "      <td>0.005761</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>158.602640</td>\n",
       "      <td>0.338479</td>\n",
       "      <td>0.534363</td>\n",
       "      <td>0.007036</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>{'colsample_bytree': 0.5, 'learning_rate': 0.1...</td>\n",
       "      <td>0.508284</td>\n",
       "      <td>0.486019</td>\n",
       "      <td>0.493675</td>\n",
       "      <td>0.495993</td>\n",
       "      <td>0.009237</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>147.076597</td>\n",
       "      <td>1.070555</td>\n",
       "      <td>0.504379</td>\n",
       "      <td>0.006795</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'colsample_bytree': 1.0, 'learning_rate': 0.1...</td>\n",
       "      <td>0.510894</td>\n",
       "      <td>0.480754</td>\n",
       "      <td>0.493752</td>\n",
       "      <td>0.495134</td>\n",
       "      <td>0.012344</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>82.670233</td>\n",
       "      <td>0.178284</td>\n",
       "      <td>0.531364</td>\n",
       "      <td>0.031439</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'colsample_bytree': 0.5, 'learning_rate': 0.1...</td>\n",
       "      <td>0.502570</td>\n",
       "      <td>0.484507</td>\n",
       "      <td>0.492704</td>\n",
       "      <td>0.493260</td>\n",
       "      <td>0.007385</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>280.786699</td>\n",
       "      <td>3.435052</td>\n",
       "      <td>0.502043</td>\n",
       "      <td>0.018619</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.9</td>\n",
       "      <td>{'colsample_bytree': 1.0, 'learning_rate': 0.1...</td>\n",
       "      <td>0.508258</td>\n",
       "      <td>0.481180</td>\n",
       "      <td>0.488980</td>\n",
       "      <td>0.492806</td>\n",
       "      <td>0.011382</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>275.020646</td>\n",
       "      <td>1.899389</td>\n",
       "      <td>0.513707</td>\n",
       "      <td>0.017897</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'colsample_bytree': 1.0, 'learning_rate': 0.1...</td>\n",
       "      <td>0.497063</td>\n",
       "      <td>0.484357</td>\n",
       "      <td>0.492210</td>\n",
       "      <td>0.491210</td>\n",
       "      <td>0.005236</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>150.587539</td>\n",
       "      <td>0.506417</td>\n",
       "      <td>0.551686</td>\n",
       "      <td>0.021217</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'colsample_bytree': 0.5, 'learning_rate': 0.1...</td>\n",
       "      <td>0.501803</td>\n",
       "      <td>0.478025</td>\n",
       "      <td>0.491433</td>\n",
       "      <td>0.490420</td>\n",
       "      <td>0.009734</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>76.316187</td>\n",
       "      <td>0.249396</td>\n",
       "      <td>0.514040</td>\n",
       "      <td>0.017734</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'colsample_bytree': 0.5, 'learning_rate': 0.1...</td>\n",
       "      <td>0.504742</td>\n",
       "      <td>0.473440</td>\n",
       "      <td>0.486752</td>\n",
       "      <td>0.488312</td>\n",
       "      <td>0.012828</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>133.788805</td>\n",
       "      <td>0.088514</td>\n",
       "      <td>0.534534</td>\n",
       "      <td>0.004496</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'colsample_bytree': 1.0, 'learning_rate': 0.1...</td>\n",
       "      <td>0.505332</td>\n",
       "      <td>0.467684</td>\n",
       "      <td>0.484594</td>\n",
       "      <td>0.485870</td>\n",
       "      <td>0.015397</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>156.214000</td>\n",
       "      <td>0.532769</td>\n",
       "      <td>0.538027</td>\n",
       "      <td>0.003298</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.9</td>\n",
       "      <td>{'colsample_bytree': 0.5, 'learning_rate': 0.1...</td>\n",
       "      <td>0.498342</td>\n",
       "      <td>0.481365</td>\n",
       "      <td>0.477763</td>\n",
       "      <td>0.485824</td>\n",
       "      <td>0.008974</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>241.393138</td>\n",
       "      <td>0.230564</td>\n",
       "      <td>0.531031</td>\n",
       "      <td>0.004026</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'colsample_bytree': 1.0, 'learning_rate': 0.1...</td>\n",
       "      <td>0.504790</td>\n",
       "      <td>0.462041</td>\n",
       "      <td>0.487932</td>\n",
       "      <td>0.484921</td>\n",
       "      <td>0.017583</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>132.607783</td>\n",
       "      <td>0.339198</td>\n",
       "      <td>0.576671</td>\n",
       "      <td>0.026966</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'colsample_bytree': 0.5, 'learning_rate': 0.1...</td>\n",
       "      <td>0.483465</td>\n",
       "      <td>0.479665</td>\n",
       "      <td>0.477493</td>\n",
       "      <td>0.480208</td>\n",
       "      <td>0.002468</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "10      85.693844      2.213753         0.544690        0.020821   \n",
       "11      89.449371      0.336864         0.525035        0.016968   \n",
       "58     145.543646      2.261844         0.500049        0.004782   \n",
       "59     145.769618      0.791158         0.524368        0.032047   \n",
       "15     158.602640      0.338479         0.534363        0.007036   \n",
       "57     147.076597      1.070555         0.504379        0.006795   \n",
       "9       82.670233      0.178284         0.531364        0.031439   \n",
       "62     280.786699      3.435052         0.502043        0.018619   \n",
       "61     275.020646      1.899389         0.513707        0.017897   \n",
       "13     150.587539      0.506417         0.551686        0.021217   \n",
       "8       76.316187      0.249396         0.514040        0.017734   \n",
       "56     133.788805      0.088514         0.534534        0.004496   \n",
       "14     156.214000      0.532769         0.538027        0.003298   \n",
       "60     241.393138      0.230564         0.531031        0.004026   \n",
       "12     132.607783      0.339198         0.576671        0.026966   \n",
       "\n",
       "   param_colsample_bytree param_learning_rate param_max_depth param_subsample  \\\n",
       "10                    0.5                 0.1               5             0.9   \n",
       "11                    0.5                 0.1               5               1   \n",
       "58                      1                 0.1               5             0.9   \n",
       "59                      1                 0.1               5               1   \n",
       "15                    0.5                 0.1              10               1   \n",
       "57                      1                 0.1               5             0.8   \n",
       "9                     0.5                 0.1               5             0.8   \n",
       "62                      1                 0.1              10             0.9   \n",
       "61                      1                 0.1              10             0.8   \n",
       "13                    0.5                 0.1              10             0.8   \n",
       "8                     0.5                 0.1               5             0.5   \n",
       "56                      1                 0.1               5             0.5   \n",
       "14                    0.5                 0.1              10             0.9   \n",
       "60                      1                 0.1              10             0.5   \n",
       "12                    0.5                 0.1              10             0.5   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "10  {'colsample_bytree': 0.5, 'learning_rate': 0.1...           0.499862   \n",
       "11  {'colsample_bytree': 0.5, 'learning_rate': 0.1...           0.504106   \n",
       "58  {'colsample_bytree': 1.0, 'learning_rate': 0.1...           0.513539   \n",
       "59  {'colsample_bytree': 1.0, 'learning_rate': 0.1...           0.500517   \n",
       "15  {'colsample_bytree': 0.5, 'learning_rate': 0.1...           0.508284   \n",
       "57  {'colsample_bytree': 1.0, 'learning_rate': 0.1...           0.510894   \n",
       "9   {'colsample_bytree': 0.5, 'learning_rate': 0.1...           0.502570   \n",
       "62  {'colsample_bytree': 1.0, 'learning_rate': 0.1...           0.508258   \n",
       "61  {'colsample_bytree': 1.0, 'learning_rate': 0.1...           0.497063   \n",
       "13  {'colsample_bytree': 0.5, 'learning_rate': 0.1...           0.501803   \n",
       "8   {'colsample_bytree': 0.5, 'learning_rate': 0.1...           0.504742   \n",
       "56  {'colsample_bytree': 1.0, 'learning_rate': 0.1...           0.505332   \n",
       "14  {'colsample_bytree': 0.5, 'learning_rate': 0.1...           0.498342   \n",
       "60  {'colsample_bytree': 1.0, 'learning_rate': 0.1...           0.504790   \n",
       "12  {'colsample_bytree': 0.5, 'learning_rate': 0.1...           0.483465   \n",
       "\n",
       "    split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "10           0.493125           0.502894         0.498626        0.004083   \n",
       "11           0.496403           0.492636         0.497716        0.004773   \n",
       "58           0.487369           0.491939         0.497617        0.011414   \n",
       "59           0.488827           0.501516         0.496953        0.005761   \n",
       "15           0.486019           0.493675         0.495993        0.009237   \n",
       "57           0.480754           0.493752         0.495134        0.012344   \n",
       "9            0.484507           0.492704         0.493260        0.007385   \n",
       "62           0.481180           0.488980         0.492806        0.011382   \n",
       "61           0.484357           0.492210         0.491210        0.005236   \n",
       "13           0.478025           0.491433         0.490420        0.009734   \n",
       "8            0.473440           0.486752         0.488312        0.012828   \n",
       "56           0.467684           0.484594         0.485870        0.015397   \n",
       "14           0.481365           0.477763         0.485824        0.008974   \n",
       "60           0.462041           0.487932         0.484921        0.017583   \n",
       "12           0.479665           0.477493         0.480208        0.002468   \n",
       "\n",
       "    rank_test_score  \n",
       "10                1  \n",
       "11                2  \n",
       "58                3  \n",
       "59                4  \n",
       "15                5  \n",
       "57                6  \n",
       "9                 7  \n",
       "62                8  \n",
       "61                9  \n",
       "13               10  \n",
       "8                11  \n",
       "56               12  \n",
       "14               13  \n",
       "60               14  \n",
       "12               15  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "GridSearch_table_plot(cv, \"subsample\", negative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phamduy\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['gridsearchCVxgboost.pkl']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(cv.best_estimator_, 'gridsearchCVxgboost.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:32:37] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[1.3202715 2.6918292 2.5183406 ... 1.9014398 1.9723109 2.7580009]\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "loaded_model = joblib.load(\"gridsearchCVxgboost.pkl\")\n",
    "result = loaded_model.predict(normed_test_data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.3202715, 2.6918292, 2.5183406, ..., 1.9014398, 1.9723109,\n",
       "       2.7580009], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_predictions = cv.predict(normed_test_data)\n",
    "test_data_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "submit = pd.read_csv(\"./../Data/sample_submit.csv\",header=None)\n",
    "submit[1] = test_data_predictions\n",
    "now = datetime.datetime.now()\n",
    "now_str = '{}_{}_{}_{}_{}'.format(now.year, now.month, now.day, now.hour, now.minute)\n",
    "submit_file = './../Data/submit/submit_{}.csv'.format(now_str)\n",
    "submit.to_csv(submit_file,header=None,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
