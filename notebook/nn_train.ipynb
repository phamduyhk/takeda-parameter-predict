{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import Imputer\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "# import theano\n",
    "# theano.config.device = 'gpu'\n",
    "# theano.config.floatX = 'float32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Score</th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col3</th>\n",
       "      <th>col4</th>\n",
       "      <th>col5</th>\n",
       "      <th>col6</th>\n",
       "      <th>col7</th>\n",
       "      <th>col8</th>\n",
       "      <th>...</th>\n",
       "      <th>col3796</th>\n",
       "      <th>col3797</th>\n",
       "      <th>col3798</th>\n",
       "      <th>col3799</th>\n",
       "      <th>col3800</th>\n",
       "      <th>col3801</th>\n",
       "      <th>col3802</th>\n",
       "      <th>col3803</th>\n",
       "      <th>col3804</th>\n",
       "      <th>col3805</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>3.475628</td>\n",
       "      <td>0</td>\n",
       "      <td>4.058</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0</td>\n",
       "      <td>10.267</td>\n",
       "      <td>0.728</td>\n",
       "      <td>4.403</td>\n",
       "      <td>0.050</td>\n",
       "      <td>...</td>\n",
       "      <td>1.067</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.115</td>\n",
       "      <td>30.395</td>\n",
       "      <td>24.541</td>\n",
       "      <td>0</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>3.601332</td>\n",
       "      <td>0</td>\n",
       "      <td>4.111</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0</td>\n",
       "      <td>8.352</td>\n",
       "      <td>0.907</td>\n",
       "      <td>4.216</td>\n",
       "      <td>0.034</td>\n",
       "      <td>...</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.227</td>\n",
       "      <td>38.508</td>\n",
       "      <td>35.038</td>\n",
       "      <td>0</td>\n",
       "      <td>3.979</td>\n",
       "      <td>0.997</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>1.935003</td>\n",
       "      <td>0</td>\n",
       "      <td>4.139</td>\n",
       "      <td>0.833</td>\n",
       "      <td>66</td>\n",
       "      <td>9.494</td>\n",
       "      <td>0.733</td>\n",
       "      <td>4.069</td>\n",
       "      <td>0.267</td>\n",
       "      <td>...</td>\n",
       "      <td>1.722</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.148</td>\n",
       "      <td>27.932</td>\n",
       "      <td>19.518</td>\n",
       "      <td>0</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>3.283663</td>\n",
       "      <td>0</td>\n",
       "      <td>4.016</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0</td>\n",
       "      <td>8.237</td>\n",
       "      <td>0.836</td>\n",
       "      <td>3.956</td>\n",
       "      <td>0.129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.124</td>\n",
       "      <td>18.993</td>\n",
       "      <td>25.403</td>\n",
       "      <td>0</td>\n",
       "      <td>0.988</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>3.409121</td>\n",
       "      <td>0</td>\n",
       "      <td>4.657</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0</td>\n",
       "      <td>35.882</td>\n",
       "      <td>0.383</td>\n",
       "      <td>4.234</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>...</td>\n",
       "      <td>2.095</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.088</td>\n",
       "      <td>44.225</td>\n",
       "      <td>15.741</td>\n",
       "      <td>0</td>\n",
       "      <td>1.595</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15</td>\n",
       "      <td>2.523486</td>\n",
       "      <td>0</td>\n",
       "      <td>4.623</td>\n",
       "      <td>0.711</td>\n",
       "      <td>66</td>\n",
       "      <td>35.891</td>\n",
       "      <td>0.581</td>\n",
       "      <td>4.154</td>\n",
       "      <td>0.327</td>\n",
       "      <td>...</td>\n",
       "      <td>3.785</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078</td>\n",
       "      <td>157.893</td>\n",
       "      <td>20.733</td>\n",
       "      <td>0</td>\n",
       "      <td>3.691</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18</td>\n",
       "      <td>0.919078</td>\n",
       "      <td>0</td>\n",
       "      <td>4.345</td>\n",
       "      <td>0.668</td>\n",
       "      <td>0</td>\n",
       "      <td>20.449</td>\n",
       "      <td>0.542</td>\n",
       "      <td>4.773</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>...</td>\n",
       "      <td>1.604</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.107</td>\n",
       "      <td>5.552</td>\n",
       "      <td>14.887</td>\n",
       "      <td>0</td>\n",
       "      <td>4.480</td>\n",
       "      <td>0.996</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>19</td>\n",
       "      <td>2.769820</td>\n",
       "      <td>0</td>\n",
       "      <td>4.380</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0</td>\n",
       "      <td>16.474</td>\n",
       "      <td>0.531</td>\n",
       "      <td>3.890</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>...</td>\n",
       "      <td>1.292</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>36.866</td>\n",
       "      <td>18.200</td>\n",
       "      <td>0</td>\n",
       "      <td>3.190</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20</td>\n",
       "      <td>2.681964</td>\n",
       "      <td>0</td>\n",
       "      <td>4.235</td>\n",
       "      <td>0.923</td>\n",
       "      <td>16</td>\n",
       "      <td>15.185</td>\n",
       "      <td>0.902</td>\n",
       "      <td>4.606</td>\n",
       "      <td>0.097</td>\n",
       "      <td>...</td>\n",
       "      <td>1.056</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>38.458</td>\n",
       "      <td>36.047</td>\n",
       "      <td>0</td>\n",
       "      <td>4.016</td>\n",
       "      <td>0.998</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>25</td>\n",
       "      <td>2.465977</td>\n",
       "      <td>0</td>\n",
       "      <td>4.214</td>\n",
       "      <td>0.522</td>\n",
       "      <td>66</td>\n",
       "      <td>3.885</td>\n",
       "      <td>0.426</td>\n",
       "      <td>4.156</td>\n",
       "      <td>0.346</td>\n",
       "      <td>...</td>\n",
       "      <td>1.726</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.349</td>\n",
       "      <td>0</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>27</td>\n",
       "      <td>3.302547</td>\n",
       "      <td>0</td>\n",
       "      <td>4.390</td>\n",
       "      <td>0.743</td>\n",
       "      <td>6</td>\n",
       "      <td>18.404</td>\n",
       "      <td>0.427</td>\n",
       "      <td>4.486</td>\n",
       "      <td>0.256</td>\n",
       "      <td>...</td>\n",
       "      <td>1.410</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>50.367</td>\n",
       "      <td>23.541</td>\n",
       "      <td>0</td>\n",
       "      <td>5.253</td>\n",
       "      <td>0.998</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>32</td>\n",
       "      <td>2.132580</td>\n",
       "      <td>0</td>\n",
       "      <td>4.161</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0</td>\n",
       "      <td>7.130</td>\n",
       "      <td>0.584</td>\n",
       "      <td>4.060</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>...</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.447</td>\n",
       "      <td>13.416</td>\n",
       "      <td>0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>34</td>\n",
       "      <td>1.673021</td>\n",
       "      <td>0</td>\n",
       "      <td>4.422</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0</td>\n",
       "      <td>19.245</td>\n",
       "      <td>0.931</td>\n",
       "      <td>4.026</td>\n",
       "      <td>0.072</td>\n",
       "      <td>...</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>72.463</td>\n",
       "      <td>51.347</td>\n",
       "      <td>0</td>\n",
       "      <td>1.077</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>35</td>\n",
       "      <td>2.323046</td>\n",
       "      <td>0</td>\n",
       "      <td>4.361</td>\n",
       "      <td>0.605</td>\n",
       "      <td>0</td>\n",
       "      <td>31.862</td>\n",
       "      <td>0.446</td>\n",
       "      <td>4.599</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>...</td>\n",
       "      <td>1.323</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.079</td>\n",
       "      <td>85.780</td>\n",
       "      <td>20.693</td>\n",
       "      <td>0</td>\n",
       "      <td>1.763</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>37</td>\n",
       "      <td>2.623353</td>\n",
       "      <td>0</td>\n",
       "      <td>4.055</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0</td>\n",
       "      <td>7.224</td>\n",
       "      <td>0.464</td>\n",
       "      <td>4.212</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>...</td>\n",
       "      <td>1.462</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.115</td>\n",
       "      <td>6.758</td>\n",
       "      <td>13.115</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>44</td>\n",
       "      <td>2.381656</td>\n",
       "      <td>0</td>\n",
       "      <td>4.079</td>\n",
       "      <td>0.621</td>\n",
       "      <td>39</td>\n",
       "      <td>11.937</td>\n",
       "      <td>0.519</td>\n",
       "      <td>4.593</td>\n",
       "      <td>0.268</td>\n",
       "      <td>...</td>\n",
       "      <td>1.494</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.123</td>\n",
       "      <td>45.247</td>\n",
       "      <td>16.647</td>\n",
       "      <td>0</td>\n",
       "      <td>1.131</td>\n",
       "      <td>0.998</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>45</td>\n",
       "      <td>2.678609</td>\n",
       "      <td>0</td>\n",
       "      <td>3.916</td>\n",
       "      <td>0.727</td>\n",
       "      <td>39</td>\n",
       "      <td>5.262</td>\n",
       "      <td>0.672</td>\n",
       "      <td>4.620</td>\n",
       "      <td>0.285</td>\n",
       "      <td>...</td>\n",
       "      <td>1.327</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>43.453</td>\n",
       "      <td>17.946</td>\n",
       "      <td>1</td>\n",
       "      <td>1.249</td>\n",
       "      <td>0.998</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>47</td>\n",
       "      <td>2.490520</td>\n",
       "      <td>0</td>\n",
       "      <td>4.362</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0</td>\n",
       "      <td>14.564</td>\n",
       "      <td>0.783</td>\n",
       "      <td>4.244</td>\n",
       "      <td>0.066</td>\n",
       "      <td>...</td>\n",
       "      <td>1.934</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.112</td>\n",
       "      <td>13.383</td>\n",
       "      <td>22.774</td>\n",
       "      <td>0</td>\n",
       "      <td>1.784</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>50</td>\n",
       "      <td>2.296446</td>\n",
       "      <td>0</td>\n",
       "      <td>3.905</td>\n",
       "      <td>0.637</td>\n",
       "      <td>0</td>\n",
       "      <td>3.391</td>\n",
       "      <td>0.490</td>\n",
       "      <td>4.614</td>\n",
       "      <td>0.141</td>\n",
       "      <td>...</td>\n",
       "      <td>1.042</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.124</td>\n",
       "      <td>3.569</td>\n",
       "      <td>12.153</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>51</td>\n",
       "      <td>0.698970</td>\n",
       "      <td>0</td>\n",
       "      <td>4.352</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0</td>\n",
       "      <td>14.041</td>\n",
       "      <td>0.404</td>\n",
       "      <td>4.590</td>\n",
       "      <td>0.036</td>\n",
       "      <td>...</td>\n",
       "      <td>1.132</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.080</td>\n",
       "      <td>17.540</td>\n",
       "      <td>15.649</td>\n",
       "      <td>0</td>\n",
       "      <td>5.651</td>\n",
       "      <td>0.998</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>52</td>\n",
       "      <td>2.327359</td>\n",
       "      <td>0</td>\n",
       "      <td>4.071</td>\n",
       "      <td>0.508</td>\n",
       "      <td>21</td>\n",
       "      <td>3.197</td>\n",
       "      <td>0.386</td>\n",
       "      <td>4.558</td>\n",
       "      <td>0.205</td>\n",
       "      <td>...</td>\n",
       "      <td>1.174</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.120</td>\n",
       "      <td>1.006</td>\n",
       "      <td>9.734</td>\n",
       "      <td>0</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>54</td>\n",
       "      <td>1.957128</td>\n",
       "      <td>0</td>\n",
       "      <td>4.473</td>\n",
       "      <td>0.752</td>\n",
       "      <td>6</td>\n",
       "      <td>19.828</td>\n",
       "      <td>0.746</td>\n",
       "      <td>4.140</td>\n",
       "      <td>0.222</td>\n",
       "      <td>...</td>\n",
       "      <td>1.737</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.104</td>\n",
       "      <td>77.378</td>\n",
       "      <td>22.546</td>\n",
       "      <td>0</td>\n",
       "      <td>2.148</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>55</td>\n",
       "      <td>2.411956</td>\n",
       "      <td>0</td>\n",
       "      <td>4.246</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0</td>\n",
       "      <td>14.887</td>\n",
       "      <td>0.576</td>\n",
       "      <td>4.736</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>...</td>\n",
       "      <td>1.264</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.106</td>\n",
       "      <td>15.977</td>\n",
       "      <td>20.967</td>\n",
       "      <td>0</td>\n",
       "      <td>5.165</td>\n",
       "      <td>0.997</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>57</td>\n",
       "      <td>3.264605</td>\n",
       "      <td>0</td>\n",
       "      <td>4.041</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0</td>\n",
       "      <td>8.213</td>\n",
       "      <td>0.786</td>\n",
       "      <td>4.592</td>\n",
       "      <td>0.152</td>\n",
       "      <td>...</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.121</td>\n",
       "      <td>15.802</td>\n",
       "      <td>24.630</td>\n",
       "      <td>0</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>59</td>\n",
       "      <td>0.204120</td>\n",
       "      <td>0</td>\n",
       "      <td>4.286</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0</td>\n",
       "      <td>4.702</td>\n",
       "      <td>0.537</td>\n",
       "      <td>4.157</td>\n",
       "      <td>0.008</td>\n",
       "      <td>...</td>\n",
       "      <td>1.268</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.072</td>\n",
       "      <td>9.871</td>\n",
       "      <td>13.356</td>\n",
       "      <td>0</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>61</td>\n",
       "      <td>1.403121</td>\n",
       "      <td>0</td>\n",
       "      <td>4.255</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0</td>\n",
       "      <td>12.017</td>\n",
       "      <td>0.767</td>\n",
       "      <td>4.056</td>\n",
       "      <td>0.048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.088</td>\n",
       "      <td>27.842</td>\n",
       "      <td>19.286</td>\n",
       "      <td>0</td>\n",
       "      <td>2.594</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>65</td>\n",
       "      <td>2.755722</td>\n",
       "      <td>0</td>\n",
       "      <td>4.487</td>\n",
       "      <td>0.915</td>\n",
       "      <td>0</td>\n",
       "      <td>16.295</td>\n",
       "      <td>0.878</td>\n",
       "      <td>4.519</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>...</td>\n",
       "      <td>1.347</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.088</td>\n",
       "      <td>40.468</td>\n",
       "      <td>45.545</td>\n",
       "      <td>0</td>\n",
       "      <td>2.278</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>66</td>\n",
       "      <td>2.158362</td>\n",
       "      <td>0</td>\n",
       "      <td>4.679</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0</td>\n",
       "      <td>28.065</td>\n",
       "      <td>0.704</td>\n",
       "      <td>3.917</td>\n",
       "      <td>0.051</td>\n",
       "      <td>...</td>\n",
       "      <td>1.757</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.088</td>\n",
       "      <td>48.227</td>\n",
       "      <td>24.852</td>\n",
       "      <td>0</td>\n",
       "      <td>3.094</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>68</td>\n",
       "      <td>1.583199</td>\n",
       "      <td>0</td>\n",
       "      <td>3.525</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.603</td>\n",
       "      <td>3.913</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.534</td>\n",
       "      <td>0</td>\n",
       "      <td>2.555</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>69</td>\n",
       "      <td>1.999565</td>\n",
       "      <td>0</td>\n",
       "      <td>4.496</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0</td>\n",
       "      <td>23.499</td>\n",
       "      <td>0.898</td>\n",
       "      <td>3.992</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>...</td>\n",
       "      <td>1.038</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.093</td>\n",
       "      <td>33.858</td>\n",
       "      <td>56.588</td>\n",
       "      <td>0</td>\n",
       "      <td>4.166</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13701</th>\n",
       "      <td>27406</td>\n",
       "      <td>3.398721</td>\n",
       "      <td>0</td>\n",
       "      <td>3.780</td>\n",
       "      <td>0.889</td>\n",
       "      <td>2</td>\n",
       "      <td>7.808</td>\n",
       "      <td>0.831</td>\n",
       "      <td>4.105</td>\n",
       "      <td>0.432</td>\n",
       "      <td>...</td>\n",
       "      <td>1.143</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.134</td>\n",
       "      <td>18.858</td>\n",
       "      <td>22.743</td>\n",
       "      <td>0</td>\n",
       "      <td>2.189</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13702</th>\n",
       "      <td>27407</td>\n",
       "      <td>1.624282</td>\n",
       "      <td>0</td>\n",
       "      <td>4.223</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0</td>\n",
       "      <td>7.885</td>\n",
       "      <td>0.538</td>\n",
       "      <td>4.231</td>\n",
       "      <td>0.022</td>\n",
       "      <td>...</td>\n",
       "      <td>1.212</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.285</td>\n",
       "      <td>12.446</td>\n",
       "      <td>0</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13703</th>\n",
       "      <td>27410</td>\n",
       "      <td>1.396199</td>\n",
       "      <td>0</td>\n",
       "      <td>4.174</td>\n",
       "      <td>0.764</td>\n",
       "      <td>6</td>\n",
       "      <td>17.377</td>\n",
       "      <td>0.738</td>\n",
       "      <td>4.605</td>\n",
       "      <td>0.319</td>\n",
       "      <td>...</td>\n",
       "      <td>1.432</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103</td>\n",
       "      <td>51.584</td>\n",
       "      <td>23.645</td>\n",
       "      <td>0</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.997</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13704</th>\n",
       "      <td>27411</td>\n",
       "      <td>2.057666</td>\n",
       "      <td>0</td>\n",
       "      <td>4.426</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0</td>\n",
       "      <td>24.371</td>\n",
       "      <td>0.390</td>\n",
       "      <td>4.044</td>\n",
       "      <td>0.086</td>\n",
       "      <td>...</td>\n",
       "      <td>1.594</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.081</td>\n",
       "      <td>34.772</td>\n",
       "      <td>16.770</td>\n",
       "      <td>0</td>\n",
       "      <td>1.775</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13705</th>\n",
       "      <td>27412</td>\n",
       "      <td>2.947973</td>\n",
       "      <td>0</td>\n",
       "      <td>4.020</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0</td>\n",
       "      <td>4.019</td>\n",
       "      <td>0.693</td>\n",
       "      <td>4.304</td>\n",
       "      <td>0.101</td>\n",
       "      <td>...</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.493</td>\n",
       "      <td>0</td>\n",
       "      <td>1.353</td>\n",
       "      <td>0.997</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13706</th>\n",
       "      <td>27413</td>\n",
       "      <td>1.767898</td>\n",
       "      <td>0</td>\n",
       "      <td>4.257</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0</td>\n",
       "      <td>17.293</td>\n",
       "      <td>0.579</td>\n",
       "      <td>4.360</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>...</td>\n",
       "      <td>1.542</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.117</td>\n",
       "      <td>24.233</td>\n",
       "      <td>23.071</td>\n",
       "      <td>0</td>\n",
       "      <td>1.928</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13707</th>\n",
       "      <td>27414</td>\n",
       "      <td>2.433930</td>\n",
       "      <td>0</td>\n",
       "      <td>4.094</td>\n",
       "      <td>0.627</td>\n",
       "      <td>6</td>\n",
       "      <td>12.138</td>\n",
       "      <td>0.592</td>\n",
       "      <td>4.563</td>\n",
       "      <td>0.327</td>\n",
       "      <td>...</td>\n",
       "      <td>1.303</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.114</td>\n",
       "      <td>44.990</td>\n",
       "      <td>16.759</td>\n",
       "      <td>0</td>\n",
       "      <td>1.237</td>\n",
       "      <td>0.998</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13708</th>\n",
       "      <td>27415</td>\n",
       "      <td>2.178401</td>\n",
       "      <td>0</td>\n",
       "      <td>3.813</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0</td>\n",
       "      <td>1.292</td>\n",
       "      <td>0.408</td>\n",
       "      <td>4.323</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.729</td>\n",
       "      <td>0</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13709</th>\n",
       "      <td>27419</td>\n",
       "      <td>1.307496</td>\n",
       "      <td>0</td>\n",
       "      <td>4.125</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0</td>\n",
       "      <td>9.170</td>\n",
       "      <td>0.641</td>\n",
       "      <td>4.473</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.761</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.096</td>\n",
       "      <td>11.843</td>\n",
       "      <td>17.295</td>\n",
       "      <td>0</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13710</th>\n",
       "      <td>27422</td>\n",
       "      <td>1.271842</td>\n",
       "      <td>0</td>\n",
       "      <td>4.006</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.264</td>\n",
       "      <td>4.078</td>\n",
       "      <td>0.027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.076</td>\n",
       "      <td>0</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13711</th>\n",
       "      <td>27423</td>\n",
       "      <td>0.875061</td>\n",
       "      <td>0</td>\n",
       "      <td>4.099</td>\n",
       "      <td>0.602</td>\n",
       "      <td>66</td>\n",
       "      <td>4.045</td>\n",
       "      <td>0.474</td>\n",
       "      <td>4.219</td>\n",
       "      <td>0.379</td>\n",
       "      <td>...</td>\n",
       "      <td>1.826</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.271</td>\n",
       "      <td>11.113</td>\n",
       "      <td>0</td>\n",
       "      <td>0.668</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13712</th>\n",
       "      <td>27426</td>\n",
       "      <td>0.698970</td>\n",
       "      <td>0</td>\n",
       "      <td>4.276</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0</td>\n",
       "      <td>18.345</td>\n",
       "      <td>0.523</td>\n",
       "      <td>4.140</td>\n",
       "      <td>0.003</td>\n",
       "      <td>...</td>\n",
       "      <td>1.446</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>44.547</td>\n",
       "      <td>15.895</td>\n",
       "      <td>0</td>\n",
       "      <td>0.917</td>\n",
       "      <td>0.998</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13713</th>\n",
       "      <td>27430</td>\n",
       "      <td>1.626340</td>\n",
       "      <td>0</td>\n",
       "      <td>4.571</td>\n",
       "      <td>0.761</td>\n",
       "      <td>0</td>\n",
       "      <td>34.644</td>\n",
       "      <td>0.626</td>\n",
       "      <td>4.064</td>\n",
       "      <td>0.099</td>\n",
       "      <td>...</td>\n",
       "      <td>1.979</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.107</td>\n",
       "      <td>56.265</td>\n",
       "      <td>26.088</td>\n",
       "      <td>0</td>\n",
       "      <td>3.935</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13714</th>\n",
       "      <td>27432</td>\n",
       "      <td>2.842110</td>\n",
       "      <td>0</td>\n",
       "      <td>4.192</td>\n",
       "      <td>0.837</td>\n",
       "      <td>2</td>\n",
       "      <td>15.931</td>\n",
       "      <td>0.711</td>\n",
       "      <td>4.429</td>\n",
       "      <td>0.036</td>\n",
       "      <td>...</td>\n",
       "      <td>1.104</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.099</td>\n",
       "      <td>29.696</td>\n",
       "      <td>21.799</td>\n",
       "      <td>0</td>\n",
       "      <td>2.710</td>\n",
       "      <td>0.998</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13715</th>\n",
       "      <td>27433</td>\n",
       "      <td>3.735503</td>\n",
       "      <td>0</td>\n",
       "      <td>4.275</td>\n",
       "      <td>0.813</td>\n",
       "      <td>6</td>\n",
       "      <td>20.218</td>\n",
       "      <td>0.573</td>\n",
       "      <td>4.484</td>\n",
       "      <td>0.156</td>\n",
       "      <td>...</td>\n",
       "      <td>1.326</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.115</td>\n",
       "      <td>16.894</td>\n",
       "      <td>29.736</td>\n",
       "      <td>0</td>\n",
       "      <td>3.964</td>\n",
       "      <td>0.998</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13716</th>\n",
       "      <td>27434</td>\n",
       "      <td>2.224274</td>\n",
       "      <td>0</td>\n",
       "      <td>4.261</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0</td>\n",
       "      <td>24.216</td>\n",
       "      <td>0.726</td>\n",
       "      <td>4.340</td>\n",
       "      <td>0.065</td>\n",
       "      <td>...</td>\n",
       "      <td>1.368</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.080</td>\n",
       "      <td>45.703</td>\n",
       "      <td>27.494</td>\n",
       "      <td>0</td>\n",
       "      <td>2.763</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13717</th>\n",
       "      <td>27435</td>\n",
       "      <td>2.088845</td>\n",
       "      <td>0</td>\n",
       "      <td>3.630</td>\n",
       "      <td>0.831</td>\n",
       "      <td>6</td>\n",
       "      <td>1.894</td>\n",
       "      <td>0.823</td>\n",
       "      <td>4.473</td>\n",
       "      <td>0.353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.611</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.134</td>\n",
       "      <td>58.124</td>\n",
       "      <td>16.642</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.928</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13718</th>\n",
       "      <td>27439</td>\n",
       "      <td>2.553155</td>\n",
       "      <td>0</td>\n",
       "      <td>4.272</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0</td>\n",
       "      <td>28.507</td>\n",
       "      <td>0.452</td>\n",
       "      <td>4.471</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>...</td>\n",
       "      <td>1.240</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>74.171</td>\n",
       "      <td>21.496</td>\n",
       "      <td>0</td>\n",
       "      <td>3.883</td>\n",
       "      <td>0.998</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13719</th>\n",
       "      <td>27440</td>\n",
       "      <td>2.069668</td>\n",
       "      <td>0</td>\n",
       "      <td>4.067</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0</td>\n",
       "      <td>10.727</td>\n",
       "      <td>0.526</td>\n",
       "      <td>4.270</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.104</td>\n",
       "      <td>8.063</td>\n",
       "      <td>18.735</td>\n",
       "      <td>0</td>\n",
       "      <td>2.101</td>\n",
       "      <td>0.998</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13720</th>\n",
       "      <td>27441</td>\n",
       "      <td>1.494155</td>\n",
       "      <td>0</td>\n",
       "      <td>4.428</td>\n",
       "      <td>0.713</td>\n",
       "      <td>0</td>\n",
       "      <td>10.734</td>\n",
       "      <td>0.585</td>\n",
       "      <td>4.290</td>\n",
       "      <td>0.011</td>\n",
       "      <td>...</td>\n",
       "      <td>1.181</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.087</td>\n",
       "      <td>25.090</td>\n",
       "      <td>17.837</td>\n",
       "      <td>0</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13721</th>\n",
       "      <td>27442</td>\n",
       "      <td>1.421604</td>\n",
       "      <td>0</td>\n",
       "      <td>3.950</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0</td>\n",
       "      <td>2.832</td>\n",
       "      <td>0.523</td>\n",
       "      <td>4.665</td>\n",
       "      <td>0.064</td>\n",
       "      <td>...</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.001</td>\n",
       "      <td>11.152</td>\n",
       "      <td>0</td>\n",
       "      <td>4.156</td>\n",
       "      <td>0.998</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13722</th>\n",
       "      <td>27443</td>\n",
       "      <td>1.750508</td>\n",
       "      <td>0</td>\n",
       "      <td>4.329</td>\n",
       "      <td>0.677</td>\n",
       "      <td>0</td>\n",
       "      <td>11.620</td>\n",
       "      <td>0.557</td>\n",
       "      <td>4.740</td>\n",
       "      <td>0.269</td>\n",
       "      <td>...</td>\n",
       "      <td>1.529</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>6.124</td>\n",
       "      <td>14.572</td>\n",
       "      <td>0</td>\n",
       "      <td>1.614</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13723</th>\n",
       "      <td>27446</td>\n",
       "      <td>2.078094</td>\n",
       "      <td>0</td>\n",
       "      <td>4.466</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0</td>\n",
       "      <td>18.604</td>\n",
       "      <td>0.589</td>\n",
       "      <td>4.665</td>\n",
       "      <td>0.022</td>\n",
       "      <td>...</td>\n",
       "      <td>1.799</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.086</td>\n",
       "      <td>30.735</td>\n",
       "      <td>18.289</td>\n",
       "      <td>0</td>\n",
       "      <td>3.215</td>\n",
       "      <td>0.997</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13724</th>\n",
       "      <td>27447</td>\n",
       "      <td>0.462398</td>\n",
       "      <td>0</td>\n",
       "      <td>4.266</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0</td>\n",
       "      <td>7.375</td>\n",
       "      <td>0.590</td>\n",
       "      <td>4.736</td>\n",
       "      <td>0.250</td>\n",
       "      <td>...</td>\n",
       "      <td>1.386</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.088</td>\n",
       "      <td>10.528</td>\n",
       "      <td>16.822</td>\n",
       "      <td>0</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13725</th>\n",
       "      <td>27448</td>\n",
       "      <td>2.551450</td>\n",
       "      <td>0</td>\n",
       "      <td>3.974</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0</td>\n",
       "      <td>5.008</td>\n",
       "      <td>0.561</td>\n",
       "      <td>4.568</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>...</td>\n",
       "      <td>1.007</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.131</td>\n",
       "      <td>2.318</td>\n",
       "      <td>14.065</td>\n",
       "      <td>0</td>\n",
       "      <td>3.942</td>\n",
       "      <td>0.998</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13726</th>\n",
       "      <td>27450</td>\n",
       "      <td>1.897627</td>\n",
       "      <td>0</td>\n",
       "      <td>4.232</td>\n",
       "      <td>0.720</td>\n",
       "      <td>6</td>\n",
       "      <td>14.717</td>\n",
       "      <td>0.767</td>\n",
       "      <td>4.166</td>\n",
       "      <td>0.183</td>\n",
       "      <td>...</td>\n",
       "      <td>1.556</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111</td>\n",
       "      <td>12.433</td>\n",
       "      <td>17.890</td>\n",
       "      <td>0</td>\n",
       "      <td>1.061</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13727</th>\n",
       "      <td>27453</td>\n",
       "      <td>1.567026</td>\n",
       "      <td>0</td>\n",
       "      <td>4.389</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0</td>\n",
       "      <td>21.853</td>\n",
       "      <td>0.667</td>\n",
       "      <td>4.193</td>\n",
       "      <td>0.017</td>\n",
       "      <td>...</td>\n",
       "      <td>1.556</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>39.003</td>\n",
       "      <td>21.724</td>\n",
       "      <td>0</td>\n",
       "      <td>1.929</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13728</th>\n",
       "      <td>27459</td>\n",
       "      <td>4.008115</td>\n",
       "      <td>0</td>\n",
       "      <td>4.510</td>\n",
       "      <td>0.817</td>\n",
       "      <td>6</td>\n",
       "      <td>25.947</td>\n",
       "      <td>0.688</td>\n",
       "      <td>4.326</td>\n",
       "      <td>0.232</td>\n",
       "      <td>...</td>\n",
       "      <td>2.292</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.081</td>\n",
       "      <td>101.934</td>\n",
       "      <td>31.747</td>\n",
       "      <td>0</td>\n",
       "      <td>2.716</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13729</th>\n",
       "      <td>27460</td>\n",
       "      <td>0.255273</td>\n",
       "      <td>0</td>\n",
       "      <td>4.490</td>\n",
       "      <td>0.689</td>\n",
       "      <td>0</td>\n",
       "      <td>25.385</td>\n",
       "      <td>0.497</td>\n",
       "      <td>4.713</td>\n",
       "      <td>0.059</td>\n",
       "      <td>...</td>\n",
       "      <td>2.227</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.079</td>\n",
       "      <td>37.596</td>\n",
       "      <td>19.484</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13730</th>\n",
       "      <td>27463</td>\n",
       "      <td>2.321184</td>\n",
       "      <td>0</td>\n",
       "      <td>4.044</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0</td>\n",
       "      <td>5.564</td>\n",
       "      <td>0.563</td>\n",
       "      <td>4.548</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.127</td>\n",
       "      <td>8.514</td>\n",
       "      <td>13.845</td>\n",
       "      <td>0</td>\n",
       "      <td>2.154</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13731 rows Ã— 3807 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID     Score  col1   col2   col3  col4    col5   col6   col7   col8  \\\n",
       "0          5  3.475628     0  4.058  0.824     0  10.267  0.728  4.403  0.050   \n",
       "1          8  3.601332     0  4.111  0.929     0   8.352  0.907  4.216  0.034   \n",
       "2          9  1.935003     0  4.139  0.833    66   9.494  0.733  4.069  0.267   \n",
       "3         12  3.283663     0  4.016  0.880     0   8.237  0.836  3.956  0.129   \n",
       "4         14  3.409121     0  4.657  0.522     0  35.882  0.383  4.234 -0.089   \n",
       "5         15  2.523486     0  4.623  0.711    66  35.891  0.581  4.154  0.327   \n",
       "6         18  0.919078     0  4.345  0.668     0  20.449  0.542  4.773 -0.013   \n",
       "7         19  2.769820     0  4.380  0.722     0  16.474  0.531  3.890 -0.056   \n",
       "8         20  2.681964     0  4.235  0.923    16  15.185  0.902  4.606  0.097   \n",
       "9         25  2.465977     0  4.214  0.522    66   3.885  0.426  4.156  0.346   \n",
       "10        27  3.302547     0  4.390  0.743     6  18.404  0.427  4.486  0.256   \n",
       "11        32  2.132580     0  4.161  0.724     0   7.130  0.584  4.060 -0.074   \n",
       "12        34  1.673021     0  4.422  0.944     0  19.245  0.931  4.026  0.072   \n",
       "13        35  2.323046     0  4.361  0.605     0  31.862  0.446  4.599 -0.084   \n",
       "14        37  2.623353     0  4.055  0.606     0   7.224  0.464  4.212 -0.177   \n",
       "15        44  2.381656     0  4.079  0.621    39  11.937  0.519  4.593  0.268   \n",
       "16        45  2.678609     0  3.916  0.727    39   5.262  0.672  4.620  0.285   \n",
       "17        47  2.490520     0  4.362  0.828     0  14.564  0.783  4.244  0.066   \n",
       "18        50  2.296446     0  3.905  0.637     0   3.391  0.490  4.614  0.141   \n",
       "19        51  0.698970     0  4.352  0.636     0  14.041  0.404  4.590  0.036   \n",
       "20        52  2.327359     0  4.071  0.508    21   3.197  0.386  4.558  0.205   \n",
       "21        54  1.957128     0  4.473  0.752     6  19.828  0.746  4.140  0.222   \n",
       "22        55  2.411956     0  4.246  0.784     0  14.887  0.576  4.736 -0.011   \n",
       "23        57  3.264605     0  4.041  0.855     0   8.213  0.786  4.592  0.152   \n",
       "24        59  0.204120     0  4.286  0.693     0   4.702  0.537  4.157  0.008   \n",
       "25        61  1.403121     0  4.255  0.825     0  12.017  0.767  4.056  0.048   \n",
       "26        65  2.755722     0  4.487  0.915     0  16.295  0.878  4.519 -0.049   \n",
       "27        66  2.158362     0  4.679  0.768     0  28.065  0.704  3.917  0.051   \n",
       "28        68  1.583199     0  3.525  0.727     0   0.000  0.603  3.913 -0.024   \n",
       "29        69  1.999565     0  4.496  0.941     0  23.499  0.898  3.992 -0.069   \n",
       "...      ...       ...   ...    ...    ...   ...     ...    ...    ...    ...   \n",
       "13701  27406  3.398721     0  3.780  0.889     2   7.808  0.831  4.105  0.432   \n",
       "13702  27407  1.624282     0  4.223  0.687     0   7.885  0.538  4.231  0.022   \n",
       "13703  27410  1.396199     0  4.174  0.764     6  17.377  0.738  4.605  0.319   \n",
       "13704  27411  2.057666     0  4.426  0.560     0  24.371  0.390  4.044  0.086   \n",
       "13705  27412  2.947973     0  4.020  0.755     0   4.019  0.693  4.304  0.101   \n",
       "13706  27413  1.767898     0  4.257  0.789     0  17.293  0.579  4.360 -0.048   \n",
       "13707  27414  2.433930     0  4.094  0.627     6  12.138  0.592  4.563  0.327   \n",
       "13708  27415  2.178401     0  3.813  0.636     0   1.292  0.408  4.323 -0.053   \n",
       "13709  27419  1.307496     0  4.125  0.771     0   9.170  0.641  4.473 -0.096   \n",
       "13710  27422  1.271842     0  4.006  0.499     0   0.000  0.264  4.078  0.027   \n",
       "13711  27423  0.875061     0  4.099  0.602    66   4.045  0.474  4.219  0.379   \n",
       "13712  27426  0.698970     0  4.276  0.656     0  18.345  0.523  4.140  0.003   \n",
       "13713  27430  1.626340     0  4.571  0.761     0  34.644  0.626  4.064  0.099   \n",
       "13714  27432  2.842110     0  4.192  0.837     2  15.931  0.711  4.429  0.036   \n",
       "13715  27433  3.735503     0  4.275  0.813     6  20.218  0.573  4.484  0.156   \n",
       "13716  27434  2.224274     0  4.261  0.815     0  24.216  0.726  4.340  0.065   \n",
       "13717  27435  2.088845     0  3.630  0.831     6   1.894  0.823  4.473  0.353   \n",
       "13718  27439  2.553155     0  4.272  0.599     0  28.507  0.452  4.471 -0.043   \n",
       "13719  27440  2.069668     0  4.067  0.717     0  10.727  0.526  4.270 -0.065   \n",
       "13720  27441  1.494155     0  4.428  0.713     0  10.734  0.585  4.290  0.011   \n",
       "13721  27442  1.421604     0  3.950  0.604     0   2.832  0.523  4.665  0.064   \n",
       "13722  27443  1.750508     0  4.329  0.677     0  11.620  0.557  4.740  0.269   \n",
       "13723  27446  2.078094     0  4.466  0.705     0  18.604  0.589  4.665  0.022   \n",
       "13724  27447  0.462398     0  4.266  0.746     0   7.375  0.590  4.736  0.250   \n",
       "13725  27448  2.551450     0  3.974  0.750     0   5.008  0.561  4.568 -0.004   \n",
       "13726  27450  1.897627     0  4.232  0.720     6  14.717  0.767  4.166  0.183   \n",
       "13727  27453  1.567026     0  4.389  0.785     0  21.853  0.667  4.193  0.017   \n",
       "13728  27459  4.008115     0  4.510  0.817     6  25.947  0.688  4.326  0.232   \n",
       "13729  27460  0.255273     0  4.490  0.689     0  25.385  0.497  4.713  0.059   \n",
       "13730  27463  2.321184     0  4.044  0.665     0   5.564  0.563  4.548 -0.015   \n",
       "\n",
       "       ...  col3796  col3797  col3798  col3799  col3800  col3801  col3802  \\\n",
       "0      ...    1.067        0      0.0    0.115   30.395   24.541        0   \n",
       "1      ...    0.934        0      0.0    0.227   38.508   35.038        0   \n",
       "2      ...    1.722        0      0.0    0.148   27.932   19.518        0   \n",
       "3      ...    0.993        0      0.0    0.124   18.993   25.403        0   \n",
       "4      ...    2.095        0      0.0    0.088   44.225   15.741        0   \n",
       "5      ...    3.785        0      0.0    0.078  157.893   20.733        0   \n",
       "6      ...    1.604        1      0.0    0.107    5.552   14.887        0   \n",
       "7      ...    1.292        0      0.0    0.090   36.866   18.200        0   \n",
       "8      ...    1.056        1      0.0    0.110   38.458   36.047        0   \n",
       "9      ...    1.726        1      0.0    0.109    0.000    9.349        0   \n",
       "10     ...    1.410        0      0.0    0.110   50.367   23.541        0   \n",
       "11     ...    0.674        0      0.0    0.101    0.447   13.416        0   \n",
       "12     ...    0.938        0      0.0    0.075   72.463   51.347        0   \n",
       "13     ...    1.323        0      0.0    0.079   85.780   20.693        0   \n",
       "14     ...    1.462        0      0.0    0.115    6.758   13.115        0   \n",
       "15     ...    1.494        0      0.0    0.123   45.247   16.647        0   \n",
       "16     ...    1.327        0      0.0    0.137   43.453   17.946        1   \n",
       "17     ...    1.934        0      0.0    0.112   13.383   22.774        0   \n",
       "18     ...    1.042        0      0.0    0.124    3.569   12.153        0   \n",
       "19     ...    1.132        0      0.0    0.080   17.540   15.649        0   \n",
       "20     ...    1.174        0      0.0    0.120    1.006    9.734        0   \n",
       "21     ...    1.737        0      0.0    0.104   77.378   22.546        0   \n",
       "22     ...    1.264        0      0.0    0.106   15.977   20.967        0   \n",
       "23     ...    1.032        0      0.0    0.121   15.802   24.630        0   \n",
       "24     ...    1.268        0      0.0    0.072    9.871   13.356        0   \n",
       "25     ...    0.840        0      0.0    0.088   27.842   19.286        0   \n",
       "26     ...    1.347        0      0.0    0.088   40.468   45.545        0   \n",
       "27     ...    1.757        0      0.0    0.088   48.227   24.852        0   \n",
       "28     ...    0.424        0      0.0    0.141    0.000    7.534        0   \n",
       "29     ...    1.038        0      0.0    0.093   33.858   56.588        0   \n",
       "...    ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "13701  ...    1.143        0      0.0    0.134   18.858   22.743        0   \n",
       "13702  ...    1.212        0      0.0    0.088    0.285   12.446        0   \n",
       "13703  ...    1.432        0      0.0    0.103   51.584   23.645        0   \n",
       "13704  ...    1.594        0      0.0    0.081   34.772   16.770        0   \n",
       "13705  ...    0.785        0      0.0    0.136    0.000   13.493        0   \n",
       "13706  ...    1.542        0      0.0    0.117   24.233   23.071        0   \n",
       "13707  ...    1.303        0      0.0    0.114   44.990   16.759        0   \n",
       "13708  ...    0.740        0      0.0    0.140    0.000    9.729        0   \n",
       "13709  ...    0.761        0      0.0    0.096   11.843   17.295        0   \n",
       "13710  ...    0.813        0      0.0    0.200    0.000    7.076        0   \n",
       "13711  ...    1.826        0      0.0    0.098    0.271   11.113        0   \n",
       "13712  ...    1.446        1      0.0    0.100   44.547   15.895        0   \n",
       "13713  ...    1.979        0      0.0    0.107   56.265   26.088        0   \n",
       "13714  ...    1.104        0      0.0    0.099   29.696   21.799        0   \n",
       "13715  ...    1.326        0      0.0    0.115   16.894   29.736        0   \n",
       "13716  ...    1.368        0      0.0    0.080   45.703   27.494        0   \n",
       "13717  ...    0.611        0      0.0    0.134   58.124   16.642        0   \n",
       "13718  ...    1.240        0      0.0    0.090   74.171   21.496        0   \n",
       "13719  ...    0.872        0      0.0    0.104    8.063   18.735        0   \n",
       "13720  ...    1.181        0      0.0    0.087   25.090   17.837        0   \n",
       "13721  ...    0.771        0      0.0    0.155    0.001   11.152        0   \n",
       "13722  ...    1.529        0      0.0    0.095    6.124   14.572        0   \n",
       "13723  ...    1.799        0      0.0    0.086   30.735   18.289        0   \n",
       "13724  ...    1.386        0      0.0    0.088   10.528   16.822        0   \n",
       "13725  ...    1.007        1      0.0    0.131    2.318   14.065        0   \n",
       "13726  ...    1.556        0      0.0    0.111   12.433   17.890        0   \n",
       "13727  ...    1.556        0      0.0    0.125   39.003   21.724        0   \n",
       "13728  ...    2.292        1      0.0    0.081  101.934   31.747        0   \n",
       "13729  ...    2.227        0      0.0    0.079   37.596   19.484        0   \n",
       "13730  ...    0.868        0      0.0    0.127    8.514   13.845        0   \n",
       "\n",
       "       col3803  col3804  col3805  \n",
       "0        0.415    0.997        0  \n",
       "1        3.979    0.997        3  \n",
       "2        0.849    0.999        0  \n",
       "3        0.988    0.998        0  \n",
       "4        1.595    0.997        0  \n",
       "5        3.691    0.997        0  \n",
       "6        4.480    0.996        4  \n",
       "7        3.190    0.998        0  \n",
       "8        4.016    0.998        2  \n",
       "9        0.411    0.999        0  \n",
       "10       5.253    0.998        2  \n",
       "11       0.532    0.997        0  \n",
       "12       1.077    0.998        0  \n",
       "13       1.763    0.998        0  \n",
       "14       0.573    0.997        0  \n",
       "15       1.131    0.998        4  \n",
       "16       1.249    0.998        4  \n",
       "17       1.784    0.997        0  \n",
       "18      -0.179    0.997        0  \n",
       "19       5.651    0.998        1  \n",
       "20       0.187    0.998        0  \n",
       "21       2.148    0.997        0  \n",
       "22       5.165    0.997        3  \n",
       "23       0.185    0.997        0  \n",
       "24       0.796    0.997        0  \n",
       "25       2.594    0.998        0  \n",
       "26       2.278    0.997        0  \n",
       "27       3.094    0.998        0  \n",
       "28       2.555    0.997        0  \n",
       "29       4.166    0.998        0  \n",
       "...        ...      ...      ...  \n",
       "13701    2.189    0.996        0  \n",
       "13702    0.788    0.997        0  \n",
       "13703    0.674    0.997        4  \n",
       "13704    1.775    0.997        0  \n",
       "13705    1.353    0.997        3  \n",
       "13706    1.928    0.997        0  \n",
       "13707    1.237    0.998        4  \n",
       "13708    0.658    0.998        0  \n",
       "13709    0.982    0.998        0  \n",
       "13710    0.895    0.997        0  \n",
       "13711    0.668    0.998        0  \n",
       "13712    0.917    0.998        2  \n",
       "13713    3.935    0.997        0  \n",
       "13714    2.710    0.998        1  \n",
       "13715    3.964    0.998        2  \n",
       "13716    2.763    0.997        0  \n",
       "13717   -0.928    0.998        0  \n",
       "13718    3.883    0.998        4  \n",
       "13719    2.101    0.998        3  \n",
       "13720    0.403    0.998        0  \n",
       "13721    4.156    0.998        2  \n",
       "13722    1.614    0.995        0  \n",
       "13723    3.215    0.997        2  \n",
       "13724    0.472    0.995        0  \n",
       "13725    3.942    0.998        2  \n",
       "13726    1.061    0.997        0  \n",
       "13727    1.929    0.998        0  \n",
       "13728    2.716    0.998        0  \n",
       "13729   -0.248    0.998        0  \n",
       "13730    2.154    0.997        0  \n",
       "\n",
       "[13731 rows x 3807 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "train_data_path =\"./../Data/train/train.csv\"\n",
    "test_data_path = \"./../Data/test/test.csv\"\n",
    "\n",
    "dataset = pd.read_csv(train_data_path,skipinitialspace=True)\n",
    "test_data = pd.read_csv(test_data_path,skipinitialspace=True)\n",
    "\n",
    "\n",
    "train_dataset = dataset.sample(frac=0.7, random_state=0)\n",
    "test_dataset = dataset.drop(train_dataset.index)\n",
    "        \n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', 'Score', 'col1', 'col2', 'col3', 'col4', 'col5', 'col6', 'col7',\n",
      "       'col8',\n",
      "       ...\n",
      "       'col3796', 'col3797', 'col3798', 'col3799', 'col3800', 'col3801',\n",
      "       'col3802', 'col3803', 'col3804', 'col3805'],\n",
      "      dtype='object', length=3807)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe for Normalization\n",
    "label = \"Score\"\n",
    "train_stats = train_dataset.describe()\n",
    "train_stats.pop(label)\n",
    "train_stats = train_stats.transpose()\n",
    "\n",
    "test_stats = test_dataset.describe()\n",
    "test_stats.pop(label)\n",
    "test_stats = test_stats.transpose()\n",
    "\n",
    "test_data_stats = test_data.describe()\n",
    "test_data_stats = test_data_stats.transpose()\n",
    "\n",
    "# pop label\n",
    "# test_goto.pop(\"keiyaku_pr\")\n",
    "train_labels = train_dataset.pop(label)\n",
    "test_labels = test_dataset.pop(label)\n",
    "\n",
    "# dataset.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13732, 3806)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalization\n",
    "def norm(train_dataset,train_stats):\n",
    "    return (train_dataset - train_stats[\"mean\"]) / train_stats[\"std\"]\n",
    "\n",
    "normed_train_data = norm(train_dataset,train_stats)\n",
    "normed_test_data = norm(test_dataset,test_stats)\n",
    "# normed_train_data = normed_train_data.dropna(axis=1)\n",
    "# normed_test_data = normed_test_data.dropna(axis=1)\n",
    "\n",
    "normed_train_data.fillna(0, inplace=True)\n",
    "normed_test_data.fillna(0, inplace=True)\n",
    "normed_test_data = norm(test_data,test_data_stats)\n",
    "normed_test_data.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "normed_test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "layer_numbers = 128\n",
    "\n",
    "# mean_squared_logarithmic_error, mean_absolute_percentage_error, mean_absolute_error\n",
    "loss = 'mean_squared_error'\n",
    "def soft_acc(y_true, y_pred):\n",
    "    return K.mean(K.equal(K.round(y_true), K.round(y_pred)))\n",
    "\n",
    "def build_model(dataset):\n",
    "    model = keras.Sequential([\n",
    "    layers.Dense(layer_numbers,input_shape= [len(train_dataset.keys())],\n",
    "                 activation = tf.nn.relu,\n",
    "                kernel_regularizer=regularizers.l2(0.01)),\n",
    "    layers.Dense(layer_numbers, activation=tf.nn.relu,\n",
    "                kernel_regularizer=regularizers.l2(0.01)),\n",
    "#     layers.Dense(layer_numbers, activation=tf.nn.relu,\n",
    "#         kernel_regularizer=regularizers.l2(0.01)),\n",
    "#     layers.Dense(layer_numbers, activation=tf.nn.relu,\n",
    "#                 kernel_regularizer=regularizers.l2(0.01)),\n",
    "#     layers.Dense(layer_numbers, activation=tf.nn.relu,\n",
    "#                 kernel_regularizer=regularizers.l2(0.01)),\n",
    "    layers.Dense(1)\n",
    "  ])\n",
    "#     optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "    optimizer='adam'\n",
    "    model.compile(\n",
    "        loss=\"mean_squared_error\",\n",
    "        optimizer=optimizer,\n",
    "        metrics=[soft_acc,\"mean_squared_error\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def build_model_dropout(dataset):\n",
    "    model = keras.Sequential([\n",
    "    # å…¥åŠ›å±¤ ã‹ã‚‰ éš ã‚Œå±¤\n",
    "    layers.Dense(layer_numbers,input_shape= [len(train_dataset.keys())],\n",
    "                 activation = tf.nn.relu),\n",
    "    layers.Dropout(rate=0.3),\n",
    "    layers.Dense(layer_numbers, activation=tf.nn.relu),\n",
    "    layers.Dropout(rate=0.2),\n",
    "    layers.Dense(layer_numbers, activation=tf.nn.relu),\n",
    "    layers.Dropout(rate=0.3),\n",
    "    layers.Dense(layer_numbers, activation=tf.nn.relu),\n",
    "    layers.Dropout(rate=0.2),\n",
    "    # éš ã‚Œå±¤ ã‹ã‚‰ å‡ºåŠ›å±¤ã€€\n",
    "    layers.Dense(1)\n",
    "  ])\n",
    "#     optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "    optimizer='adam'\n",
    "    model.compile(\n",
    "        loss=\"mean_squared_error\",\n",
    "        optimizer=optimizer,\n",
    "        metrics=[soft_acc,\"mean_squared_error\"],\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist[\"epoch\"] = history.epoch\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.plot(hist[\"epoch\"], hist[\"loss\"], label=\"Train\")\n",
    "    plt.plot(hist[\"epoch\"], hist[\"val_loss\"], label=\"Val\")\n",
    "    plt.legend()\n",
    "#     plt.ylim([0, 5])\n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"acc\")\n",
    "    plt.plot(hist[\"epoch\"], hist[\"soft_acc\"], label=\"Train\")\n",
    "    plt.plot(hist[\"epoch\"], hist[\"val_soft_acc\"], label=\"Val\")\n",
    "    plt.legend()\n",
    "#     plt.ylim([0, 20])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚¨ãƒãƒƒã‚¯ãŒçµ‚ã‚ã‚‹ã”ã¨ã«ãƒ‰ãƒƒãƒˆã‚’ä¸€ã¤å‡ºåŠ›ã™ã‚‹ã“ã¨ã§é€²æ—ã‚’è¡¨ç¤º\n",
    "class PrintDot(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"100\")\n",
    "        print(\".\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 128)               487296    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 503,937\n",
      "Trainable params: 503,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.74993914],\n",
       "       [1.175977  ],\n",
       "       [0.49693352],\n",
       "       [1.2706431 ],\n",
       "       [0.88770705],\n",
       "       [0.11365783],\n",
       "       [1.019623  ],\n",
       "       [1.2954646 ],\n",
       "       [0.9164804 ],\n",
       "       [0.629891  ]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build model\n",
    "# model = build_model_dropout(normed_train_data)\n",
    "\n",
    "model = build_model(normed_train_data)\n",
    "\n",
    "# if os.path.isfile(\"param_goto.hdf5\"):\n",
    "#     model.load_weights('param_goto.hdf5')\n",
    "    \n",
    "model.summary()\n",
    "\n",
    "# example trainning\n",
    "example_batch = normed_train_data[:10]\n",
    "example_result = model.predict(example_batch)\n",
    "example_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7689 samples, validate on 1923 samples\n",
      "Epoch 1/400\n",
      "100\n",
      ". - 8s - loss: 0.9436 - soft_acc: 0.5197 - mean_squared_error: 0.4562 - val_loss: 1.0148 - val_soft_acc: 0.4930 - val_mean_squared_error: 0.5506\n",
      "Epoch 2/400\n",
      ". - 7s - loss: 0.8933 - soft_acc: 0.5162 - mean_squared_error: 0.4573 - val_loss: 0.9819 - val_soft_acc: 0.5031 - val_mean_squared_error: 0.5705\n",
      "Epoch 3/400\n",
      ". - 8s - loss: 0.8561 - soft_acc: 0.5083 - mean_squared_error: 0.4663 - val_loss: 1.2119 - val_soft_acc: 0.4259 - val_mean_squared_error: 0.8344\n",
      "Epoch 4/400\n",
      ". - 7s - loss: 0.8253 - soft_acc: 0.5140 - mean_squared_error: 0.4664 - val_loss: 1.0513 - val_soft_acc: 0.4594 - val_mean_squared_error: 0.7089\n",
      "Epoch 5/400\n",
      ". - 7s - loss: 0.7700 - soft_acc: 0.5201 - mean_squared_error: 0.4495 - val_loss: 0.8726 - val_soft_acc: 0.4848 - val_mean_squared_error: 0.5778\n",
      "Epoch 6/400\n",
      ". - 7s - loss: 0.7409 - soft_acc: 0.5238 - mean_squared_error: 0.4572 - val_loss: 0.8211 - val_soft_acc: 0.4921 - val_mean_squared_error: 0.5476\n",
      "Epoch 7/400\n",
      ". - 6s - loss: 0.6979 - soft_acc: 0.5224 - mean_squared_error: 0.4399 - val_loss: 0.9229 - val_soft_acc: 0.4648 - val_mean_squared_error: 0.6833\n",
      "Epoch 8/400\n",
      ". - 6s - loss: 0.6552 - soft_acc: 0.5277 - mean_squared_error: 0.4283 - val_loss: 0.7662 - val_soft_acc: 0.4969 - val_mean_squared_error: 0.5461\n",
      "Epoch 9/400\n",
      ". - 6s - loss: 0.6379 - soft_acc: 0.5372 - mean_squared_error: 0.4260 - val_loss: 0.7394 - val_soft_acc: 0.5063 - val_mean_squared_error: 0.5350\n",
      "Epoch 10/400\n",
      ". - 6s - loss: 0.6125 - soft_acc: 0.5341 - mean_squared_error: 0.4189 - val_loss: 0.7077 - val_soft_acc: 0.5087 - val_mean_squared_error: 0.5212\n",
      "Epoch 11/400\n",
      ". - 6s - loss: 0.5989 - soft_acc: 0.5311 - mean_squared_error: 0.4210 - val_loss: 0.7043 - val_soft_acc: 0.4877 - val_mean_squared_error: 0.5320\n",
      "Epoch 12/400\n",
      ". - 6s - loss: 0.5838 - soft_acc: 0.5409 - mean_squared_error: 0.4182 - val_loss: 0.6863 - val_soft_acc: 0.4942 - val_mean_squared_error: 0.5317\n",
      "Epoch 13/400\n",
      ". - 6s - loss: 0.5544 - soft_acc: 0.5508 - mean_squared_error: 0.4023 - val_loss: 0.6555 - val_soft_acc: 0.5232 - val_mean_squared_error: 0.5079\n",
      "Epoch 14/400\n",
      ". - 6s - loss: 0.5651 - soft_acc: 0.5338 - mean_squared_error: 0.4189 - val_loss: 0.7040 - val_soft_acc: 0.4839 - val_mean_squared_error: 0.5608\n",
      "Epoch 15/400\n",
      ". - 6s - loss: 0.5532 - soft_acc: 0.5399 - mean_squared_error: 0.4132 - val_loss: 0.7049 - val_soft_acc: 0.4997 - val_mean_squared_error: 0.5671\n",
      "Epoch 16/400\n",
      ". - 7s - loss: 0.5436 - soft_acc: 0.5454 - mean_squared_error: 0.4092 - val_loss: 0.6820 - val_soft_acc: 0.4921 - val_mean_squared_error: 0.5525\n",
      "Epoch 17/400\n",
      ". - 7s - loss: 0.5528 - soft_acc: 0.5346 - mean_squared_error: 0.4167 - val_loss: 0.6945 - val_soft_acc: 0.4758 - val_mean_squared_error: 0.5671\n",
      "Epoch 18/400\n",
      ". - 7s - loss: 0.5299 - soft_acc: 0.5366 - mean_squared_error: 0.4057 - val_loss: 0.6298 - val_soft_acc: 0.5137 - val_mean_squared_error: 0.5034\n",
      "Epoch 19/400\n",
      ". - 7s - loss: 0.5231 - soft_acc: 0.5451 - mean_squared_error: 0.4020 - val_loss: 0.6374 - val_soft_acc: 0.5009 - val_mean_squared_error: 0.5192\n",
      "Epoch 20/400\n",
      ". - 7s - loss: 0.5238 - soft_acc: 0.5471 - mean_squared_error: 0.4025 - val_loss: 0.7007 - val_soft_acc: 0.4812 - val_mean_squared_error: 0.5822\n",
      "Epoch 21/400\n",
      ". - 7s - loss: 0.5176 - soft_acc: 0.5400 - mean_squared_error: 0.4025 - val_loss: 0.6182 - val_soft_acc: 0.5213 - val_mean_squared_error: 0.5057\n",
      "Epoch 22/400\n",
      ". - 8s - loss: 0.5014 - soft_acc: 0.5484 - mean_squared_error: 0.3916 - val_loss: 0.6311 - val_soft_acc: 0.5048 - val_mean_squared_error: 0.5242\n",
      "Epoch 23/400\n",
      ". - 7s - loss: 0.5055 - soft_acc: 0.5469 - mean_squared_error: 0.3948 - val_loss: 0.6118 - val_soft_acc: 0.5193 - val_mean_squared_error: 0.5034\n",
      "Epoch 24/400\n",
      ". - 6s - loss: 0.5103 - soft_acc: 0.5459 - mean_squared_error: 0.4026 - val_loss: 0.6426 - val_soft_acc: 0.4945 - val_mean_squared_error: 0.5197\n",
      "Epoch 25/400\n",
      ". - 6s - loss: 0.5011 - soft_acc: 0.5496 - mean_squared_error: 0.3903 - val_loss: 0.6111 - val_soft_acc: 0.5065 - val_mean_squared_error: 0.5012\n",
      "Epoch 26/400\n",
      ". - 7s - loss: 0.4997 - soft_acc: 0.5431 - mean_squared_error: 0.3932 - val_loss: 0.6294 - val_soft_acc: 0.5070 - val_mean_squared_error: 0.5237\n",
      "Epoch 27/400\n",
      ". - 6s - loss: 0.5062 - soft_acc: 0.5459 - mean_squared_error: 0.3995 - val_loss: 0.6243 - val_soft_acc: 0.5159 - val_mean_squared_error: 0.5215\n",
      "Epoch 28/400\n",
      ". - 7s - loss: 0.4938 - soft_acc: 0.5413 - mean_squared_error: 0.3921 - val_loss: 0.6025 - val_soft_acc: 0.5048 - val_mean_squared_error: 0.5026\n",
      "Epoch 29/400\n",
      ". - 6s - loss: 0.4883 - soft_acc: 0.5439 - mean_squared_error: 0.3871 - val_loss: 0.6126 - val_soft_acc: 0.5063 - val_mean_squared_error: 0.5130\n",
      "Epoch 30/400\n",
      ". - 7s - loss: 0.5026 - soft_acc: 0.5485 - mean_squared_error: 0.4011 - val_loss: 0.6036 - val_soft_acc: 0.5080 - val_mean_squared_error: 0.5009\n",
      "Epoch 31/400\n",
      ". - 9s - loss: 0.4946 - soft_acc: 0.5498 - mean_squared_error: 0.3963 - val_loss: 0.6153 - val_soft_acc: 0.5015 - val_mean_squared_error: 0.5183\n",
      "Epoch 32/400\n",
      ". - 7s - loss: 0.4824 - soft_acc: 0.5544 - mean_squared_error: 0.3866 - val_loss: 0.6125 - val_soft_acc: 0.5126 - val_mean_squared_error: 0.5184\n",
      "Epoch 33/400\n",
      ". - 7s - loss: 0.4841 - soft_acc: 0.5438 - mean_squared_error: 0.3910 - val_loss: 0.6063 - val_soft_acc: 0.4862 - val_mean_squared_error: 0.5150\n",
      "Epoch 34/400\n",
      ". - 7s - loss: 0.4724 - soft_acc: 0.5548 - mean_squared_error: 0.3798 - val_loss: 0.6082 - val_soft_acc: 0.4988 - val_mean_squared_error: 0.5167\n",
      "Epoch 35/400\n",
      ". - 7s - loss: 0.4865 - soft_acc: 0.5462 - mean_squared_error: 0.3931 - val_loss: 0.6099 - val_soft_acc: 0.5055 - val_mean_squared_error: 0.5153\n",
      "Epoch 36/400\n",
      ". - 7s - loss: 0.4873 - soft_acc: 0.5466 - mean_squared_error: 0.3937 - val_loss: 0.5923 - val_soft_acc: 0.5220 - val_mean_squared_error: 0.4992\n",
      "Epoch 37/400\n",
      ". - 8s - loss: 0.4827 - soft_acc: 0.5447 - mean_squared_error: 0.3893 - val_loss: 0.6574 - val_soft_acc: 0.4950 - val_mean_squared_error: 0.5645\n",
      "Epoch 38/400\n",
      ". - 9s - loss: 0.4871 - soft_acc: 0.5389 - mean_squared_error: 0.3932 - val_loss: 0.6098 - val_soft_acc: 0.5125 - val_mean_squared_error: 0.5184\n",
      "Epoch 39/400\n",
      ". - 7s - loss: 0.4774 - soft_acc: 0.5513 - mean_squared_error: 0.3857 - val_loss: 0.6091 - val_soft_acc: 0.4851 - val_mean_squared_error: 0.5187\n",
      "Epoch 40/400\n",
      ". - 8s - loss: 0.4751 - soft_acc: 0.5521 - mean_squared_error: 0.3842 - val_loss: 0.5944 - val_soft_acc: 0.5169 - val_mean_squared_error: 0.5064\n",
      "Epoch 41/400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-25578412f671>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                     validation_split = 0.2, verbose=2, callbacks=[\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#                         early_stop,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                         PrintDot()])\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;31m# Callbacks batch end.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mbatch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mmake_logs\u001b[0;34m(model, outputs, mode, prefix)\u001b[0m\n\u001b[1;32m    144\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'metrics_names'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mmetrics_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m     \u001b[0;31m# Add metric names from layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m       \u001b[0mmetrics_names\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metrics\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[0mmetrics_names\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metrics\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mlayers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;31m# `CheckpointableBase` manages the `_layers` attributes and does filtering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;31m# over it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlayers\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInputLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mlayers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    424\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     return checkpointable_layer_utils.filter_empty_layer_containers(\n\u001b[0;32m--> 426\u001b[0;31m         self._layers)\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/checkpointable/layer_utils.py\u001b[0m in \u001b[0;36mfilter_empty_layer_containers\u001b[0;34m(layer_list)\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0mfiltered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mis_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m       \u001b[0mfiltered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"layers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/checkpointable/layer_utils.py\u001b[0m in \u001b[0;36mis_layer\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     28\u001b[0m   return (hasattr(obj, \"call\")\n\u001b[1;32m     29\u001b[0m           \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"build\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m           and hasattr(obj, \"variables\"))\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mvariables\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1241\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m     \"\"\"\n\u001b[0;32m-> 1243\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mweights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    645\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \"\"\"\n\u001b[0;32m--> 647\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnon_trainable_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mnon_trainable_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    632\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnon_trainable_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m       \u001b[0mnested\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_children_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'non_trainable_weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_non_trainable_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnested\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_gather_children_attribute\u001b[0;34m(self, attribute)\u001b[0m\n\u001b[1;32m   1644\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_layers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1645\u001b[0m       return list(itertools.chain.from_iterable(\n\u001b[0;32m-> 1646\u001b[0;31m           getattr(layer, attribute) for layer in self._layers))\n\u001b[0m\u001b[1;32m   1647\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1644\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_layers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1645\u001b[0m       return list(itertools.chain.from_iterable(\n\u001b[0;32m-> 1646\u001b[0;31m           getattr(layer, attribute) for layer in self._layers))\n\u001b[0m\u001b[1;32m   1647\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# callback when NG\n",
    "EPOCHS = 400\n",
    "\n",
    "start_time = time.time()\n",
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "history = model.fit(normed_train_data, train_labels, epochs=EPOCHS, shuffle = True,\n",
    "                    validation_split = 0.2, verbose=2, callbacks=[\n",
    "#                         early_stop,\n",
    "                        PrintDot()])\n",
    "\n",
    "end_time = time.time()\n",
    "excute_time = end_time-start_time\n",
    "print(\"excute time: {}\".format(excute_time))\n",
    "plot_history(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_string = model.to_json()\n",
    "with open('mode.json', 'w') as outfile:\n",
    "    json.dump(json_string, outfile)\n",
    "\n",
    "#save weight\n",
    "model.save_weights('param.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>soft_acc</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_soft_acc</th>\n",
       "      <th>val_mean_squared_error</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.546123</td>\n",
       "      <td>0.362365</td>\n",
       "      <td>1.391371</td>\n",
       "      <td>4.735461</td>\n",
       "      <td>0.341530</td>\n",
       "      <td>2.026575</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.269883</td>\n",
       "      <td>0.419202</td>\n",
       "      <td>0.911121</td>\n",
       "      <td>2.776093</td>\n",
       "      <td>0.470458</td>\n",
       "      <td>0.755673</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.492174</td>\n",
       "      <td>0.454386</td>\n",
       "      <td>0.744940</td>\n",
       "      <td>2.172369</td>\n",
       "      <td>0.435963</td>\n",
       "      <td>0.658106</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.055438</td>\n",
       "      <td>0.465090</td>\n",
       "      <td>0.696978</td>\n",
       "      <td>2.123741</td>\n",
       "      <td>0.396858</td>\n",
       "      <td>0.908015</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.789891</td>\n",
       "      <td>0.472352</td>\n",
       "      <td>0.669119</td>\n",
       "      <td>1.591225</td>\n",
       "      <td>0.496243</td>\n",
       "      <td>0.570705</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.580148</td>\n",
       "      <td>0.483359</td>\n",
       "      <td>0.634977</td>\n",
       "      <td>1.516084</td>\n",
       "      <td>0.451332</td>\n",
       "      <td>0.646240</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.296056</td>\n",
       "      <td>0.505936</td>\n",
       "      <td>0.499784</td>\n",
       "      <td>1.279330</td>\n",
       "      <td>0.492486</td>\n",
       "      <td>0.546750</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.160264</td>\n",
       "      <td>0.509754</td>\n",
       "      <td>0.473843</td>\n",
       "      <td>1.205607</td>\n",
       "      <td>0.495048</td>\n",
       "      <td>0.566285</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.091808</td>\n",
       "      <td>0.510345</td>\n",
       "      <td>0.484779</td>\n",
       "      <td>1.125905</td>\n",
       "      <td>0.502391</td>\n",
       "      <td>0.548772</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.016843</td>\n",
       "      <td>0.508789</td>\n",
       "      <td>0.470480</td>\n",
       "      <td>1.106244</td>\n",
       "      <td>0.471141</td>\n",
       "      <td>0.593350</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  soft_acc  mean_squared_error  val_loss  val_soft_acc  \\\n",
       "0  4.546123  0.362365            1.391371  4.735461      0.341530   \n",
       "1  3.269883  0.419202            0.911121  2.776093      0.470458   \n",
       "2  2.492174  0.454386            0.744940  2.172369      0.435963   \n",
       "3  2.055438  0.465090            0.696978  2.123741      0.396858   \n",
       "4  1.789891  0.472352            0.669119  1.591225      0.496243   \n",
       "5  1.580148  0.483359            0.634977  1.516084      0.451332   \n",
       "6  1.296056  0.505936            0.499784  1.279330      0.492486   \n",
       "7  1.160264  0.509754            0.473843  1.205607      0.495048   \n",
       "8  1.091808  0.510345            0.484779  1.125905      0.502391   \n",
       "9  1.016843  0.508789            0.470480  1.106244      0.471141   \n",
       "\n",
       "   val_mean_squared_error  epoch  \n",
       "0                2.026575      0  \n",
       "1                0.755673      1  \n",
       "2                0.658106      2  \n",
       "3                0.908015      3  \n",
       "4                0.570705      4  \n",
       "5                0.646240      5  \n",
       "6                0.546750      6  \n",
       "7                0.566285      7  \n",
       "8                0.548772      8  \n",
       "9                0.593350      9  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist[\"epoch\"] = history.epoch\n",
    "hist.tail()\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must be the same size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-2d202e5eecb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormed_test_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'True Values'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Predictions'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, data, **kwargs)\u001b[0m\n\u001b[1;32m   2860\u001b[0m         \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlinewidths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2861\u001b[0m         verts=verts, edgecolors=edgecolors, **({\"data\": data} if data\n\u001b[0;32m-> 2862\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2863\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2864\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1808\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1810\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, **kwargs)\u001b[0m\n\u001b[1;32m   4180\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4181\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4182\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x and y must be the same size\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must be the same size"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prediction\n",
    "test_predictions = model.predict(normed_test_data).flatten()\n",
    "\n",
    "plt.scatter(test_labels, test_predictions)\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.axis('equal')\n",
    "plt.axis('square')\n",
    "plt.xlim([0,plt.xlim()[1]])\n",
    "plt.ylim([0,plt.ylim()[1]])\n",
    "_ = plt.plot([-100, 100], [-100, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = test_predictions - test_labels\n",
    "plt.hist(error, bins = 25)\n",
    "plt.xlabel(\"Prediction Error\")\n",
    "_ = plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_predictions = model.predict(normed_test_data)\n",
    "test_data_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "submit = pd.read_csv(\"./Data/sample_submit.csv\",header=None)\n",
    "submit[1] = test_data_predictions\n",
    "now = datetime.datetime.now()\n",
    "now_str = '{}_{}_{}_{}_{}'.format(now.year, now.month, now.day, now.hour, now.minute)\n",
    "submit_file = './Data/submit/submit_{}.csv'.format(now_str)\n",
    "submit.to_csv(submit_file,header=None,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights, biases = model.layers[0].get_weights()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1,b1 = model.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2,b2 = model.layers[1].get_weights()\n",
    "w2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w3,b3 = model.layers[2].get_weights()\n",
    "w3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = []\n",
    "biases = []\n",
    "feature_number = 8\n",
    "layer_number = 64\n",
    "weight= np.eye(feature_number,feature_number)\n",
    "bias = np.eye(layer_number)\n",
    "for i in range(3):\n",
    "    w,b = model.layers[i].get_weights()\n",
    "    weights.append(w)\n",
    "    biases.append(b)\n",
    "# for item in weights:\n",
    "    weight = np.dot(weight,item)\n",
    "for item in biases:\n",
    "    bias = np.dot(bias,item)\n",
    "np.save('weight_{}feature'.format(feature_number),weight)\n",
    "np.save('bias_{}feature'.format(feature_number),weight)\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
